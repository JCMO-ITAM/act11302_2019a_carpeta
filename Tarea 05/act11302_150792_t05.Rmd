#Octavio Fuentes Ortiz 150792

```{r}
data <- read.csv("~/Escritorio/CA3_allstateclaim_data.csv")
head(data)
```

$$
\iota_{tj} = 
\begin{cases}
1, & \text{ poliza }j, \text{ siniestrada en }t\\
0, & \text{ e.o.c. },
\end{cases}
$$
para $t=2005,2006,2007$ y $j=1,\ldots,J_t$.

```{r}
data <- as.data.frame(data)
data[,"iota"] <- 0
data[which(data$Claim_Amount>0),"iota"] <- 1
table(data[,c("Calendar_Year","iota")])
```

La suscripcion $J_t$ para los tres añoos, es:

```{r}
table(data$Calendar_Year)
```

El número de siniestros en esos años, $N_t$, definido como:

$$
N_t=\sum_{j=1}^{J_t}\iota_{tj},
$$

```{r}
table(data[which(data$iota==1),"Calendar_Year"])
```

Dado que cada póliza puede tener a lo más un siniestro en cada periodo de operación (en este caso), el modelo pertinente para $N_t$ puede ser:

$$
N_t \sim \text{Bin}(n|J_t,\theta_t),
$$
Reconocidiendo que cada periodo de operación puede tener una incidencia en siniestralidad distinta:

```{r}
theta_est <- as.matrix(table(data[which(data$iota==1),"Calendar_Year"])) / as.matrix(table(data$Calendar_Year))
theta_est <- as.data.frame(theta_est)
colnames(theta_est) <- c("Theta_ML")
theta_est <- ts(theta_est,start = c(2005), frequency = 1)
theta_est
plot(theta_est)
```

## Severidades

Respecto a la severidad individual, consideramos la variable $X_{tj}$ condicional en $\iota_{tj}=1$, i.e. consideramos los datos con `Claim_Amount>0`.

Esto nos brinda la oportunidad de obtener info respecto al monto agregado de siniestros, $S_t$, definido como:

\begin{eqnarray}
S_{t} & = & \sum_{j=1}^{J_t} X_{tj}\iota_{tj} \nonumber \\
      & = & \sum_{i=1}^{N_t} X_{ti}\mathbb{I}(X_{ti}>0), \nonumber
\end{eqnarray}

donde, 

* $J_t$ representa el tamanio de la suscripcion, y

* $N_t$ representa la frecencia de siniestros,

ambas cantidades en $t$.

Recordemos que aunque $N_t$ la podemos medir en términos absolutos, en realidad su info es referida a $J_t$ (esto directamente en el caso de enfoque de riesgo individual).

En este caso, los montos agregados de siniestros _anio-con-anio_, son:

```{r}
data_x <- data[which(data$iota==1),"Claim_Amount"]
data_t <- data[which(data$iota==1),"Calendar_Year"]
(S_t <- rowsum(data_x, data_t))
```


```{r}
S_t <- ts(S_t,start = c(2005), frequency = 1)
plot(S_t)
```

La informacion desagregada de las $(x_{ti})_{i=1}^{n_t}$, _periodo-a-periodo_ puede verse de la siguiente forma:

###2005

```{r}
data_x1 <- data[which(data$iota==1 & data$Calendar_Year==2005),"Claim_Amount"]
summary(data_x1)
```

```{r}
hist(data_x1,100)
```    

###2006

```{r}
data_x2 <- data[which(data$iota==1 & data$Calendar_Year==2006),"Claim_Amount"]
summary(data_x2)
```

```{r}
hist(data_x2,100)
```    


###2007

```{r}
data_x3 <- data[which(data$iota==1 & data$Calendar_Year==2007),"Claim_Amount"]
summary(data_x3)
```

```{r}
hist(data_x3,100)
```  

### Observaciones:
* El valor maximo (i.e. rango entre el `maximo muestral` y el `tercer cuantil muestral`) muestra una dentencia a la baja.

* El `monto mediano individual muestral` es en $2005$ y $2006$ semejante, mientras que en $2007$ los datos muestran un repunte en aproximadamente $4$ unidades.

* En todos los años, el `primer cuantil muestral` es semejante en nivel.

```{r}
data_x <- data[which(data$iota==1),c("Calendar_Year","Claim_Amount")]
data_x$Calendar_Year <- as.factor(data_x$Calendar_Year)
colnames(data_x)
if(!require('ggplot2')){install.packages("ggplot2")}
library("ggplot2")
ggplot(data = data_x, aes(x = Calendar_Year, y = Claim_Amount)) +
  geom_boxplot(aes(fill = Calendar_Year), width = 0.8) + theme_bw()
```

# Modelacion  {.tabset .tabset-fade .tabset-pills}

Empezaremos modelando los datos,periodo-periodo:
$$
\{n_t,(x_{ti})_{i=1}^{n_t}\},
$$
sujeto a $J_t$, para $t=2005,2006,2007$, sujeto a $J_y$ de la manera mas simple:

* **Homogeneidad** en $t$, i.e.

\begin{eqnarray}
N_t & \sim & \text{Bin}(n|J_t,\theta), \nonumber \\
x_{ti} & \sim & \text{Exp}(x|\lambda), \nonumber
\end{eqnarray}
con $0<\theta<1$ y $\lambda>0$ parametros desconocidos.

* **Simetria estocastica** en $t$ e $i=1,\ldots,n_t$, expresada en la forma de `independencia estocastica` entre `periodos` y `entre polizas`.


## Observaciones:
* Los parámetros $\theta$ y $\lambda$ son los mismos para todo $t$ (este es un supuesto no sustentable, dados los comentarios anteriores).

* El supuesto de independencia estocástica no es necesariamente sustentable, al menos respecto a $t$.

* El supuesto distribucional para las $X_{tj}$ tampoco es necesariamente sustentable, porque las caracteristicas de cambio en las $x_{tj}$ a traves del tiempo $t$ indica que la distribucion subyacente para $x_{tj}$ es recomendable tenga tres parametros (**mas adelante regresaremos a esto**).

## Parámetros:

Con base en los datos:
$$
\{n_t,(x_{ti})_{i=1}^{n_t}\},
$$
buscaremos encontrar los parametros adecuados:
$$
(\theta,\lambda) \in (0,1)\times (0,\infty),
$$

## Verosimilitud

La verosimilitud para $(\theta,\lambda)$ dados los `datos` queda expresada como,
\begin{eqnarray}
lik(\theta,\lambda|\text{datos}) & = & p(n_{2005},(x_{2005,i})_{i=1}^{n_{2005}},n_{2006},(x_{2006,i})_{i=1}^{n_{2006}},n_{2007},(x_{2007,i})_{i=1}^{n_{2007}}|\theta,\lambda) \nonumber \\
& = & \prod_{t=2005}^{2007}p(n_{t},(x_{t,i})_{i=1}^{n_{t}}|\theta,\lambda)\nonumber \\
& = & \prod_{t=2005}^{2007}\text{Bin}(n_{t}|J_t,\theta) \times \prod_{t=2005}^{2007}\prod_{i=1}^{n_t}\text{Exp}(x_{t,i}|\lambda)\nonumber \\
& = & lik\left(\theta|(J_t,n_t)_{t=2005}^{2007}\right) \times lik\left(\lambda|(x_{t,i})_{t=2005,i=1}^{2007,n_t}\right).\nonumber
\end{eqnarray}

La expresión anterior se manifiesta el supuesto de separabilidad.

```{r}
theta_grid <- seq(0,1,0.01)
lambda_grid <- seq(0,1000,1)
if(!require('lattice')){install.packages("lattice")}
library("lattice")
data_n <- sum(data$iota)
data_J <- nrow(data)
data_xsum <- sum(data[which(data$iota==1),"Claim_Amount"])
theta <- data_n/data_J; lambda <- (data_xsum / data_n)
loglikelihood <- function(theta,lambda,data_n,data_J,data_xsum){
  loglik.theta <- data_n*log(theta) + (data_J-data_n)*log(1-theta)
  loglik.lambda <- data_n*log(lambda) -lambda*data_xsum
  loglik <- loglik.theta + loglik.lambda
  return(loglik)
}
loglikelihood(theta,lambda,data_n,data_J,data_xsum)
thetalambda_grid <- expand.grid( x = theta_grid, y = lambda_grid)
dim(thetalambda_grid)
loglik_grid <- matrix(NaN,ncol=1,nrow=nrow(thetalambda_grid))
G <- nrow(thetalambda_grid)
g <- 1
for(g in 1:G){loglik_grid[g] <- loglikelihood(thetalambda_grid[g,1],thetalambda_grid[g,2],data_n,data_J,data_xsum)}

```

```{r}
if(!require('plot3D')){install.packages("plot3D")}
library("plot3D")
h<-1
H <- nrow(thetalambda_grid)

for(h in 1:H){
  if(loglik_grid[h]==-Inf){
  loglik_grid[h] <-loglik_grid[101100] 
}}
scatter3D(thetalambda_grid[,1],thetalambda_grid[,2],loglik_grid/1000000,theta=140,phi=10,clab="Mapa de intensidad (Log-Verosimilitud/1000000)",xlab="Theta",ylab="Lambda",zlab="Log-Verosimilitud",ticktype="detailed")

```

Como podemos observar en la gráfica, los valores de theta y lambda en donde la Log-Verosimilitud alcanza sus valores máximos son valores cercanos a cero para ambos parámetros, por lo que podemos concluir que los estimadores de Máxima Verosimilitud son theta y lambda ambos cercanos a cero.

```
if(!require('plot3D')){install.packages("plot3D")}
library("plot3D")

scatter3D(thetalambda_grid[,1], thetalambda_grid[,2], loglik_grid, 
          pch = 18, cex = 2, 
          theta = 20, phi = 20, ticktype = "detailed",
          xlab = "theta", ylab = "lambda", zlab = "log-lik",  
          surf = list(x = thetalambda_grid[,1], y = thetalambda_grid[,2], z = loglik_grid,  
                      facets = NA, fit = fitpoints), main = "")
```

## Estimación

De esta forma, el modelo especifico mas compatible con los datos, es:

$$
p\left(N_t,(X_{ti})_{i=1}^{N_t}\right)=\text{Bin}(n|J_t,\theta^{*}) \prod_{i=1}^{n}\text{Exp}(x|\lambda^{*}),
$$
con 
\begin{eqnarray}
\theta^{*} & = &\arg\max_{\theta\in (0,1)}lik\left(\theta|(J_t,n_t)_{t=2005}^{2007}\right) \nonumber \\
\lambda^{*} & = &\arg\max_{\lambda\in (0,\infty)}lik\left(\lambda|(x_{t,i})_{t=2005,i=1}^{2007,n_t}\right) \nonumber.
\end{eqnarray}


```{r}
theta_star <- data_n/data_J
lambda_star <- (data_xsum / data_n)^(-1)
theta_star; lambda_star
```

Es decir, las masas de probabilidades/densidades de estos componentes:

**Masas de probabilidades**
```{r}
J <- 300000
theta_sim <- seq(0,J,by = 1)
ptheta_sim <- dbinom(theta_sim,J,1-theta_star)
plot(theta_sim,ptheta_sim)
```

**Densidad**
```{r}
lambda_sim <- seq(0, 5000, length.out=1000)
lambda_frame <- data.frame(x=lambda_sim, px=dexp(lambda_sim, rate=lambda_star))
if(!require('ggplot2')){install.packages("ggplots")}
library("ggplot2")
ggplot(lambda_frame, aes(x=x, y=px)) + geom_line()
```

* La variablidad que estas dos mediciones de probabilidad describen se conoce como variablidad o incertidumbre aleatoria. Esto es porque hacen referencia a la realización de $N$ y de las $X_i$, solamente, considerando un modelo especifico dado --en este caso $\text{Bin}(n|J,\theta)$ y $\text{Exp}(x|\lambda)$--.

**Estos parametros resumen la informacion contenida en el conjunto de datos de tres anio de operacion: `2005`, `2006` y `2007`.**

# Agregacion de riesgos `2008` - Enfoque 1

Con base en la info de los tres años de opración observados (`2005`, `2006` y `2007`) tenemos el objetivo de cuantificar la incertidumbre hacia el periodo de operación `2008`. 

## Enfoque 1

$$
P(X_{2008,j})=\theta^*\delta_{\{0\}}(x)+(1-\theta^*)\text{Exp(x|\lambda^{*})},
$$
para $j=1,\ldots,J_{2008}$.

Fijamos $M$ el numero de datos a simular para `2008`, asi como la suscripcion para `2008`:

```{r}
set.seed(1)

M <- 1000
J_2008 <- 350000
datos_pred_2008 <- matrix(NA, ncol=J_2008+1,nrow=M)

dpredca3 <- function(J_2008,lambda_star,theta_star){
  datos_sim <- matrix(NA,ncol=J_2008,nrow=1)
  j<-1
  for(j in 1:J_2008){
  z <- rbinom(1,1,theta_star)
  if(z==0){
    datos_sim[1,j] <- 0
    }else{
    datos_sim[1,j] <- rexp(1,lambda_star)
    }
  }
  return(datos_sim)
}

# dpredca3(J_2008,lambda_star,theta_star)

m <- 1
for(m in 1:M){
  datos_pred_2008[m,1:J_2008] <- 
    sim <- dpredca3(J_2008,lambda_star,theta_star)
  datos_pred_2008[m,(J_2008+1)] <- sum(datos_pred_2008[m,1:J_2008])
}

# summary(datos_pred_2008[m,(J_2008+1)])
```

## Enfoque 2

$$
P(N_{2008})=\text{Bin}(n|J_20018,\theta^{*}),
$$

$$
P(X_{2008,i})=\text{Exp}(x|\lambda^{*}),
$$

```{r}
m<-1
for(m in 1:M){
  S_sim <- matrix(NA,ncol=1,nrow=M)
  N_sim <- matrix(NA,ncol=1,nrow=M)
  n_sim <- rbinom(1,J_2008,theta_star)
  x_sim <- rexp(n_sim,lambda_star)
  N_sim[m,1] <- n_sim
  S_sim[m,1] <- sum(x_sim)
}
# summary(S_sim)
```